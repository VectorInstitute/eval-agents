{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 03: Multi-Turn Conversations & Evaluation\n\nThis notebook demonstrates multi-turn conversation capabilities and\nhow to evaluate the agent on the DeepSearchQA benchmark.\n\n## Learning Objectives\n\n- Understand how ADK manages multi-turn conversations via sessions\n- Use the `DeepSearchQAEvaluator` for systematic evaluation\n- Analyze evaluation results with rich visualizations\n- Understand evaluation metrics for research agents"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Load environment and configure rich console\n",
    "import uuid\n",
    "\n",
    "from aieng.agent_evals import (\n",
    "    create_console,\n",
    "    display_evaluation_result,\n",
    "    display_metrics_table,\n",
    "    display_success,\n",
    ")\n",
    "from aieng.agent_evals.knowledge_agent import (\n",
    "    DeepSearchQADataset,\n",
    "    DeepSearchQAEvaluator,\n",
    "    KnowledgeGroundedAgent,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "from rich.panel import Panel\n",
    "from rich.table import Table\n",
    "\n",
    "\n",
    "console = create_console()\n",
    "load_dotenv(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Multi-Turn Conversations with ADK\n\nThe `KnowledgeGroundedAgent` uses Google ADK's built-in session management via `InMemorySessionService`.\nWhen you pass a `session_id` to `answer_async()`, ADK maintains conversation history automatically.\n\nKey points:\n- Each unique `session_id` creates a separate conversation thread\n- ADK tracks all messages, tool calls, and context within that session\n- No manual history tracking needed - ADK handles it internally"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent and demonstrate multi-turn conversation\n",
    "agent = KnowledgeGroundedAgent()\n",
    "\n",
    "# Create a session ID for multi-turn conversation\n",
    "session_id = str(uuid.uuid4())\n",
    "\n",
    "console.print(\n",
    "    Panel(\n",
    "        f\"[cyan]Session ID:[/cyan] {session_id}\\n\\nADK will track conversation history for this session automatically.\",\n",
    "        title=\"üó®Ô∏è New Session Created\",\n",
    "        border_style=\"green\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First turn - ask a question\n",
    "response1 = await agent.answer_async(\"What is the capital of France?\", session_id=session_id)\n",
    "console.print(Panel(response1.text, title=\"Turn 1: Capital of France\", border_style=\"blue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second turn - follow-up question (ADK remembers the context)\n",
    "response2 = await agent.answer_async(\"What is its population?\", session_id=session_id)\n",
    "console.print(Panel(response2.text, title=\"Turn 2: Population (follow-up)\", border_style=\"blue\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Session Management in Applications\n\nFor web applications (like Gradio), you can store a session ID in the app's state:\n\n```python\n# In a Gradio app handler:\nif \"session_id\" not in session_state:\n    session_state[\"session_id\"] = str(uuid.uuid4())\n\nresponse = await agent.answer_async(query, session_id=session_state[\"session_id\"])\n```\n\nSee `gradio_app.py` for a complete example."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For more details on ADK sessions, see:\n",
    "# https://google.github.io/adk-docs/sessions/\n",
    "\n",
    "display_success(\"Multi-turn conversation demo complete!\", console=console)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running DeepSearchQA Evaluation\n",
    "\n",
    "The `DeepSearchQAEvaluator` provides a systematic way to evaluate the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluator using the existing agent\n",
    "evaluator = DeepSearchQAEvaluator(agent)\n",
    "\n",
    "display_success(f\"Dataset size: {len(evaluator.dataset)} examples\", console=console)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a small sample\n",
    "console.print(\"[bold]üî¨ Running evaluation on 3 examples...[/bold]\\n\")\n",
    "\n",
    "console.print(\"[dim]Evaluating...[/dim]\")\n",
    "results = await evaluator.evaluate_sample_async(n=3, random_state=42)\n",
    "\n",
    "display_success(f\"Completed {len(results)} evaluations\", console=console)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results using the display utility\n",
    "console.print(\"\\n[bold]üìã Evaluation Results[/bold]\\n\")\n",
    "\n",
    "for result in results:\n",
    "    contains_answer = result.ground_truth.lower() in result.prediction.lower()\n",
    "    display_evaluation_result(\n",
    "        example_id=result.example_id,\n",
    "        problem=result.problem,\n",
    "        ground_truth=result.ground_truth,\n",
    "        prediction=result.prediction,\n",
    "        sources_used=result.sources_used,\n",
    "        search_queries=result.search_queries,\n",
    "        contains_answer=contains_answer,\n",
    "        console=console,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyzing Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for analysis\n",
    "df = evaluator.results_to_dataframe(results)\n",
    "\n",
    "# Calculate metrics\n",
    "containment_correct = sum(1 for r in results if r.ground_truth.lower() in r.prediction.lower())\n",
    "containment_accuracy = containment_correct / len(results) * 100\n",
    "\n",
    "metrics = {\n",
    "    \"Total Examples\": len(results),\n",
    "    \"Containment Accuracy\": f\"{containment_accuracy:.1f}%\",\n",
    "    \"Avg Sources Used\": df[\"sources_used\"].mean(),\n",
    "    \"Avg Search Queries\": df[\"search_queries\"].apply(len).mean(),\n",
    "}\n",
    "\n",
    "display_metrics_table(metrics, title=\"Evaluation Metrics\", console=console)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding Evaluation Metrics\n",
    "\n",
    "For research agents, we care about:\n",
    "\n",
    "1. **Answer Correctness**: Does the prediction match the ground truth?\n",
    "2. **Source Quality**: Are the sources relevant and authoritative?\n",
    "3. **Comprehensiveness**: Did the agent find all necessary information?\n",
    "4. **Search Efficiency**: How many searches were needed?\n",
    "\n",
    "DeepSearchQA specifically measures:\n",
    "- **Precision**: Quality of the answer\n",
    "- **Recall**: Completeness of the answer (for list-type questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual correctness check with better display\n",
    "def check_answer_contains_ground_truth(prediction: str, ground_truth: str) -> bool:\n",
    "    \"\"\"Check if prediction contains the ground truth answer.\"\"\"\n",
    "    return ground_truth.lower() in prediction.lower()\n",
    "\n",
    "\n",
    "# Check our results\n",
    "console.print(\"\\n[bold]üìä Correctness Check[/bold]\\n\")\n",
    "\n",
    "result_table = Table(show_header=True, header_style=\"bold cyan\")\n",
    "result_table.add_column(\"Example\", style=\"cyan\")\n",
    "result_table.add_column(\"Status\", style=\"white\")\n",
    "result_table.add_column(\"Expected\", style=\"dim\")\n",
    "\n",
    "for result in results:\n",
    "    contains = check_answer_contains_ground_truth(result.prediction, result.ground_truth)\n",
    "    status = \"[green]‚úì MATCH[/green]\" if contains else \"[yellow]‚úó NO MATCH[/yellow]\"\n",
    "    result_table.add_row(\n",
    "        str(result.example_id),\n",
    "        status,\n",
    "        result.ground_truth[:40] + \"...\" if len(result.ground_truth) > 40 else result.ground_truth,\n",
    "    )\n",
    "\n",
    "console.print(result_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploring Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get examples from a specific category\n",
    "dataset = DeepSearchQADataset()\n",
    "categories = dataset.get_categories()\n",
    "\n",
    "cat_table = Table(title=\"üìÅ Available Categories\", show_header=True, header_style=\"bold green\")\n",
    "cat_table.add_column(\"Category\", style=\"white\")\n",
    "cat_table.add_column(\"Count\", style=\"cyan\", justify=\"right\")\n",
    "\n",
    "for cat in sorted(categories):\n",
    "    count = len(dataset.get_by_category(cat))\n",
    "    cat_table.add_row(cat, str(count))\n",
    "\n",
    "console.print(cat_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nIn this notebook, you learned:\n\n1. How ADK manages multi-turn conversations via `InMemorySessionService`\n2. How to use `session_id` for conversation continuity\n3. How to run systematic evaluations with `DeepSearchQAEvaluator`\n4. How to analyze evaluation results with rich visualizations\n5. Key metrics for evaluating research agents\n\n## Next Steps\n\n- Run the Gradio app for interactive testing\n- Experiment with different models (gemini-2.5-pro vs flash)\n- Try the async evaluator for larger-scale evaluation\n- Implement LLM-as-judge evaluation for more nuanced correctness checking"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\n",
    "    Panel(\n",
    "        \"[green]‚úì[/green] Notebook complete!\\n\\n\"\n",
    "        \"[cyan]Next:[/cyan] Run [bold]gradio_app.py[/bold] for interactive testing.\",\n",
    "        title=\"üéâ Done\",\n",
    "        border_style=\"green\",\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
