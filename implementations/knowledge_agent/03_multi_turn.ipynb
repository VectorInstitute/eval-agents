{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03: Multi-Turn Conversations & Evaluation\n",
    "\n",
    "This notebook demonstrates multi-turn conversation management and\n",
    "how to evaluate the agent on the DeepSearchQA benchmark.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Manage multi-turn conversations with session state\n",
    "- Use the `DeepSearchQAEvaluator` for systematic evaluation\n",
    "- Analyze evaluation results with rich visualizations\n",
    "- Understand evaluation metrics for research agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Load environment and configure rich console\n",
    "from aieng.agent_evals import (\n",
    "    create_console,\n",
    "    display_evaluation_result,\n",
    "    display_info,\n",
    "    display_metrics_table,\n",
    "    display_success,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "console = create_console()\n",
    "load_dotenv(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multi-Turn Session Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aieng.agent_evals.knowledge_agent import ConversationSession\n",
    "from rich.panel import Panel\n",
    "from rich.table import Table\n",
    "\n",
    "\n",
    "# Create a session\n",
    "session = ConversationSession()\n",
    "\n",
    "console.print(\n",
    "    Panel(\n",
    "        f\"[cyan]Session ID:[/cyan] {session.session_id}\",\n",
    "        title=\"üó®Ô∏è New Session Created\",\n",
    "        border_style=\"green\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a multi-turn conversation\n",
    "session.add_user_message(\"What is the capital of France?\")\n",
    "session.add_assistant_message(\"The capital of France is Paris.\")\n",
    "\n",
    "session.add_user_message(\"What is its population?\")\n",
    "session.add_assistant_message(\"Paris has a population of about 2.1 million in the city proper.\")\n",
    "\n",
    "# Display the conversation\n",
    "console.print(\"[bold]üìù Conversation History[/bold]\\n\")\n",
    "console.print(Panel(session.get_history_as_text(), border_style=\"blue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get history as structured data\n",
    "history = session.get_history()\n",
    "\n",
    "history_table = Table(title=\"üìä Structured History\", show_header=True, header_style=\"bold cyan\")\n",
    "history_table.add_column(\"Role\", style=\"cyan\")\n",
    "history_table.add_column(\"Content\", style=\"white\")\n",
    "\n",
    "for msg in history:\n",
    "    role_style = \"green\" if msg[\"role\"] == \"user\" else \"blue\"\n",
    "    history_table.add_row(\n",
    "        f\"[{role_style}]{msg['role'].upper()}[/{role_style}]\",\n",
    "        msg[\"content\"][:50] + \"...\" if len(msg[\"content\"]) > 50 else msg[\"content\"],\n",
    "    )\n",
    "\n",
    "console.print(history_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Sessions with Gradio State\n",
    "\n",
    "In Gradio apps, use `get_or_create_session` to manage sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aieng.agent_evals.knowledge_agent import get_or_create_session\n",
    "\n",
    "\n",
    "# Simulate Gradio state\n",
    "gradio_state = {}\n",
    "\n",
    "# First turn - creates a new session\n",
    "session1 = get_or_create_session(gradio_state)\n",
    "session1.add_user_message(\"Hello!\")\n",
    "display_info(f\"Created session: {session1.session_id}\", console=console)\n",
    "\n",
    "# Second turn - retrieves existing session\n",
    "session2 = get_or_create_session(gradio_state)\n",
    "display_info(f\"Retrieved session: {session2.session_id}\", console=console)\n",
    "display_success(f\"Same session: {session1 is session2}\", console=console)\n",
    "display_info(f\"Messages in session: {len(session2)}\", console=console)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running DeepSearchQA Evaluation\n",
    "\n",
    "The `DeepSearchQAEvaluator` provides a systematic way to evaluate the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aieng.agent_evals.knowledge_agent import (\n",
    "    DeepSearchQADataset,\n",
    "    DeepSearchQAEvaluator,\n",
    "    KnowledgeGroundedAgent,\n",
    ")\n",
    "\n",
    "\n",
    "# Create agent and evaluator\n",
    "with console.status(\"[cyan]Initializing agent and evaluator...[/cyan]\", spinner=\"dots\"):\n",
    "    agent = KnowledgeGroundedAgent()\n",
    "    evaluator = DeepSearchQAEvaluator(agent)\n",
    "\n",
    "display_success(f\"Dataset size: {len(evaluator.dataset)} examples\", console=console)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a small sample\n",
    "console.print(\"[bold]üî¨ Running evaluation on 3 examples...[/bold]\\n\")\n",
    "\n",
    "console.print(\"[dim]Evaluating...[/dim]\")\n",
    "results = await evaluator.evaluate_sample_async(n=3, random_state=42)\n",
    "\n",
    "display_success(f\"Completed {len(results)} evaluations\", console=console)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results using the display utility\n",
    "console.print(\"\\n[bold]üìã Evaluation Results[/bold]\\n\")\n",
    "\n",
    "for result in results:\n",
    "    contains_answer = result.ground_truth.lower() in result.prediction.lower()\n",
    "    display_evaluation_result(\n",
    "        example_id=result.example_id,\n",
    "        problem=result.problem,\n",
    "        ground_truth=result.ground_truth,\n",
    "        prediction=result.prediction,\n",
    "        sources_used=result.sources_used,\n",
    "        search_queries=result.search_queries,\n",
    "        contains_answer=contains_answer,\n",
    "        console=console,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyzing Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for analysis\n",
    "df = evaluator.results_to_dataframe(results)\n",
    "\n",
    "# Calculate metrics\n",
    "containment_correct = sum(1 for r in results if r.ground_truth.lower() in r.prediction.lower())\n",
    "containment_accuracy = containment_correct / len(results) * 100\n",
    "\n",
    "metrics = {\n",
    "    \"Total Examples\": len(results),\n",
    "    \"Containment Accuracy\": f\"{containment_accuracy:.1f}%\",\n",
    "    \"Avg Sources Used\": df[\"sources_used\"].mean(),\n",
    "    \"Avg Search Queries\": df[\"search_queries\"].apply(len).mean(),\n",
    "}\n",
    "\n",
    "display_metrics_table(metrics, title=\"Evaluation Metrics\", console=console)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding Evaluation Metrics\n",
    "\n",
    "For research agents, we care about:\n",
    "\n",
    "1. **Answer Correctness**: Does the prediction match the ground truth?\n",
    "2. **Source Quality**: Are the sources relevant and authoritative?\n",
    "3. **Comprehensiveness**: Did the agent find all necessary information?\n",
    "4. **Search Efficiency**: How many searches were needed?\n",
    "\n",
    "DeepSearchQA specifically measures:\n",
    "- **Precision**: Quality of the answer\n",
    "- **Recall**: Completeness of the answer (for list-type questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual correctness check with better display\n",
    "def check_answer_contains_ground_truth(prediction: str, ground_truth: str) -> bool:\n",
    "    \"\"\"Check if prediction contains the ground truth answer.\"\"\"\n",
    "    return ground_truth.lower() in prediction.lower()\n",
    "\n",
    "\n",
    "# Check our results\n",
    "console.print(\"\\n[bold]üìä Correctness Check[/bold]\\n\")\n",
    "\n",
    "result_table = Table(show_header=True, header_style=\"bold cyan\")\n",
    "result_table.add_column(\"Example\", style=\"cyan\")\n",
    "result_table.add_column(\"Status\", style=\"white\")\n",
    "result_table.add_column(\"Expected\", style=\"dim\")\n",
    "\n",
    "for result in results:\n",
    "    contains = check_answer_contains_ground_truth(result.prediction, result.ground_truth)\n",
    "    status = \"[green]‚úì MATCH[/green]\" if contains else \"[yellow]‚úó NO MATCH[/yellow]\"\n",
    "    result_table.add_row(\n",
    "        str(result.example_id),\n",
    "        status,\n",
    "        result.ground_truth[:40] + \"...\" if len(result.ground_truth) > 40 else result.ground_truth,\n",
    "    )\n",
    "\n",
    "console.print(result_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploring Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get examples from a specific category\n",
    "dataset = DeepSearchQADataset()\n",
    "categories = dataset.get_categories()\n",
    "\n",
    "cat_table = Table(title=\"üìÅ Available Categories\", show_header=True, header_style=\"bold green\")\n",
    "cat_table.add_column(\"Category\", style=\"white\")\n",
    "cat_table.add_column(\"Count\", style=\"cyan\", justify=\"right\")\n",
    "\n",
    "for cat in sorted(categories):\n",
    "    count = len(dataset.get_by_category(cat))\n",
    "    cat_table.add_row(cat, str(count))\n",
    "\n",
    "console.print(cat_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. How to manage multi-turn conversations with `ConversationSession`\n",
    "2. How to use `get_or_create_session` for Gradio integration\n",
    "3. How to run systematic evaluations with `DeepSearchQAEvaluator`\n",
    "4. How to analyze evaluation results with rich visualizations\n",
    "5. Key metrics for evaluating research agents\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Run the Gradio app for interactive testing\n",
    "- Experiment with different models (gemini-2.5-pro vs flash)\n",
    "- Try the async evaluator for larger-scale evaluation\n",
    "- Implement LLM-as-judge evaluation for more nuanced correctness checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\n",
    "    Panel(\n",
    "        \"[green]‚úì[/green] Notebook complete!\\n\\n\"\n",
    "        \"[cyan]Next:[/cyan] Run [bold]gradio_app.py[/bold] for interactive testing.\",\n",
    "        title=\"üéâ Done\",\n",
    "        border_style=\"green\",\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
