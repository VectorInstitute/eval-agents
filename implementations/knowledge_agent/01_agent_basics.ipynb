{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-intro",
   "metadata": {},
   "source": [
    "# 01: Knowledge-Grounded QA Agent\n",
    "\n",
    "This notebook introduces the **Knowledge-Grounded QA Agent** - a research assistant that uses\n",
    "Google Search, web fetching, and intelligent planning to answer complex questions with citations.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Agent Architecture** - How the agent uses planning, tools, and reflection\n",
    "2. **Tools** - Google Search, web fetching, file operations\n",
    "3. **Running the Agent** - Ask questions and see it work\n",
    "4. **Evaluation** - Run a single-sample evaluation with the LLM-as-judge\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Set `GOOGLE_API_KEY` in your `.env` file\n",
    "- Run `uv sync` to install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - all imports at the top\n",
    "from aieng.agent_evals.knowledge_agent import (\n",
    "    DeepSearchQADataset,\n",
    "    DeepSearchQAJudge,\n",
    "    KnowledgeGroundedAgent,\n",
    ")\n",
    "from aieng.agent_evals.knowledge_agent.agent import SYSTEM_INSTRUCTIONS\n",
    "from aieng.agent_evals.knowledge_agent.notebook import run_with_display\n",
    "from dotenv import load_dotenv\n",
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "from rich.panel import Panel\n",
    "from rich.table import Table\n",
    "\n",
    "\n",
    "load_dotenv(verbose=True)\n",
    "console = Console(width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-arch-intro",
   "metadata": {},
   "source": "## 1. Agent Architecture\n\nThe `KnowledgeGroundedAgent` is built on Google's Agent Development Kit (ADK) and uses a **ReAct loop**\n(Reasoning + Acting) with **PlanReAct planning**.\n\n### Key Components\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    KnowledgeGroundedAgent                       │\n├─────────────────────────────────────────────────────────────────┤\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────────┐  │\n│  │  Planning   │───▶│  Execution  │───▶│    Reflection       │  │\n│  │ (PlanReAct) │    │   (Tools)   │    │  (Update Plan)      │  │\n│  └─────────────┘    └─────────────┘    └─────────────────────┘  │\n│         │                  │                     │              │\n│         ▼                  ▼                     ▼              │\n│  ┌─────────────────────────────────────────────────────────────┐│\n│  │                      Tools                                  ││\n│  │  • google_search  - Find URLs via web search                ││\n│  │  • web_fetch      - Fetch HTML pages and PDFs               ││\n│  │  • fetch_file     - Download data files (CSV, XLSX)         ││\n│  │  • grep_file      - Search within downloaded files          ││\n│  │  • read_file      - Read sections of downloaded files       ││\n│  └─────────────────────────────────────────────────────────────┘│\n└─────────────────────────────────────────────────────────────────┘\n```\n\n### How It Works\n\n1. **Planning**: The agent creates a research plan with explicit steps using PlanReAct\n2. **Execution**: Each step is executed using available tools\n3. **Reflection**: After each step, the agent reflects on findings and may update the plan\n4. **Synthesis**: Final step combines all findings into a comprehensive answer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-create-agent",
   "metadata": {},
   "outputs": [],
   "source": "# Create the agent\n\nagent = KnowledgeGroundedAgent(enable_planning=True)\n\n# Display configuration\nconfig_table = Table(title=\"Agent Configuration\", show_header=False)\nconfig_table.add_column(\"Setting\", style=\"cyan\")\nconfig_table.add_column(\"Value\", style=\"white\")\nconfig_table.add_row(\"Model\", agent.model)\nconfig_table.add_row(\"Planning\", \"PlanReAct\")\nconfig_table.add_row(\"Planning Enabled\", str(agent.enable_planning))\n\nconsole.print(config_table)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-tools-intro",
   "metadata": {},
   "source": [
    "## 2. Available Tools\n",
    "\n",
    "The agent has access to these tools for research:\n",
    "\n",
    "| Tool | Purpose | Example Use |\n",
    "|------|---------|-------------|\n",
    "| `google_search` | Find relevant URLs | Finding news articles, official sources |\n",
    "| `web_fetch` | Get full page content | Reading articles, PDFs, documentation |\n",
    "| `fetch_file` | Download data files | Getting CSV/XLSX datasets |\n",
    "| `grep_file` | Search within files | Finding specific data in large files |\n",
    "| `read_file` | Read file sections | Extracting specific parts of documents |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-show-instructions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the system instructions that guide the agent\n",
    "# Show first part of instructions\n",
    "instructions_preview = SYSTEM_INSTRUCTIONS[:2000] + \"\\n\\n[... truncated for display ...]\"\n",
    "\n",
    "console.print(\n",
    "    Panel(\n",
    "        Markdown(instructions_preview),\n",
    "        title=\"System Instructions (Preview)\",\n",
    "        border_style=\"blue\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-demo-intro",
   "metadata": {},
   "source": [
    "## 3. Running the Agent\n",
    "\n",
    "Let's ask the agent a question and observe how it works. We'll ask about a recent event\n",
    "that requires web search to answer correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-run-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question with live progress display\n",
    "question = \"When was the highest single day snowfall recorded in Toronto?\"\n",
    "\n",
    "# Run the agent with live display showing plan and tool calls\n",
    "response = await run_with_display(agent, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zzl9o9cfuc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the final answer\n",
    "console.print(\n",
    "    Panel(\n",
    "        response.text,\n",
    "        title=\"Answer\",\n",
    "        border_style=\"cyan\",\n",
    "        subtitle=f\"Duration: {response.total_duration_ms / 1000:.1f}s | Sources: {len(response.sources)}\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-show-plan",
   "metadata": {},
   "outputs": [],
   "source": "# View the research plan that was created\nplan = response.plan\n\nplan_table = Table(title=\"Research Plan\")\nplan_table.add_column(\"#\", style=\"cyan\", width=3)\nplan_table.add_column(\"Step\", style=\"white\")\nplan_table.add_column(\"Type\", style=\"dim\")\nplan_table.add_column(\"Status\", style=\"green\")\n\nfor step in plan.steps:\n    status_icon = {\"completed\": \"✓\", \"skipped\": \"○\", \"failed\": \"✗\"}.get(step.status, \"?\")\n    plan_table.add_row(\n        str(step.step_id),\n        step.description[:60] + \"...\" if len(step.description) > 60 else step.description,\n        step.step_type,\n        f\"{status_icon} {step.status}\",\n    )\n\nconsole.print(plan_table)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-show-tools",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the tool calls made during execution\n",
    "if response.tool_calls:\n",
    "    tools_table = Table(title=\"Tool Calls\")\n",
    "    tools_table.add_column(\"#\", style=\"dim\", width=3)\n",
    "    tools_table.add_column(\"Tool\", style=\"cyan\")\n",
    "    tools_table.add_column(\"Arguments\", style=\"white\")\n",
    "\n",
    "    for i, tc in enumerate(response.tool_calls[:15], 1):  # Show first 15\n",
    "        tool_name = tc.get(\"name\", \"unknown\")\n",
    "        args = tc.get(\"args\", {})\n",
    "        args_str = str(args)[:60] + \"...\" if len(str(args)) > 60 else str(args)\n",
    "        tools_table.add_row(str(i), tool_name, args_str)\n",
    "\n",
    "    if len(response.tool_calls) > 15:\n",
    "        tools_table.add_row(\"...\", f\"({len(response.tool_calls) - 15} more)\", \"\")\n",
    "\n",
    "    console.print(tools_table)\n",
    "else:\n",
    "    console.print(\"[dim]No tool calls recorded[/dim]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-show-sources",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the sources used\n",
    "if response.sources:\n",
    "    sources_table = Table(title=\"Sources\")\n",
    "    sources_table.add_column(\"#\", style=\"dim\", width=3)\n",
    "    sources_table.add_column(\"URL\", style=\"blue\")\n",
    "\n",
    "    # Deduplicate sources by URL\n",
    "    seen_urls = set()\n",
    "    for _, src in enumerate(response.sources, 1):\n",
    "        if src.uri and src.uri not in seen_urls:\n",
    "            seen_urls.add(src.uri)\n",
    "            url_display = src.uri[:80] + \"...\" if len(src.uri) > 80 else src.uri\n",
    "            sources_table.add_row(str(len(seen_urls)), url_display)\n",
    "        if len(seen_urls) >= 10:  # Show first 10 unique sources\n",
    "            break\n",
    "\n",
    "    console.print(sources_table)\n",
    "else:\n",
    "    console.print(\"[dim]No sources recorded[/dim]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-eval-intro",
   "metadata": {},
   "source": [
    "## 4. Single-Sample Evaluation\n",
    "\n",
    "The **DeepSearchQA** benchmark contains 896 research questions for evaluating knowledge agents.\n",
    "Let's evaluate the agent on a single sample using the **LLM-as-judge** approach.\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "- **Precision**: Did the agent's answer only contain correct information?\n",
    "- **Recall**: Did the agent find all the required information?\n",
    "- **F1 Score**: Harmonic mean of precision and recall\n",
    "- **Outcome**: `fully_correct`, `partially_correct`, `correct_with_extraneous`, or `fully_incorrect`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DeepSearchQA dataset\n",
    "dataset = DeepSearchQADataset()\n",
    "\n",
    "console.print(f\"Loaded [cyan]{len(dataset)}[/cyan] examples\")\n",
    "console.print(f\"Categories: [cyan]{len(dataset.get_categories())}[/cyan]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pick-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick an example from Finance & Economics category\n",
    "finance_examples = dataset.get_by_category(\"Finance & Economics\")\n",
    "example = finance_examples[0]  # Pick first one for reproducibility\n",
    "\n",
    "console.print(\n",
    "    Panel(\n",
    "        f\"[bold]ID:[/bold] {example.example_id}\\n\"\n",
    "        f\"[bold]Category:[/bold] {example.problem_category}\\n\"\n",
    "        f\"[bold]Answer Type:[/bold] {example.answer_type}\\n\\n\"\n",
    "        f\"[bold cyan]Question:[/bold cyan]\\n{example.problem}\\n\\n\"\n",
    "        f\"[bold yellow]Ground Truth:[/bold yellow]\\n{example.answer}\",\n",
    "        title=\"Test Example\",\n",
    "        border_style=\"blue\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-run-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent on this example with live progress\n",
    "eval_response = await run_with_display(agent, example.problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8gue0y17h",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the agent's answer\n",
    "console.print(\n",
    "    Panel(\n",
    "        eval_response.text,\n",
    "        title=\"Agent's Answer\",\n",
    "        border_style=\"cyan\",\n",
    "        subtitle=f\"Duration: {eval_response.total_duration_ms / 1000:.1f}s\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-judge-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the LLM-as-judge to evaluate the answer\n",
    "\n",
    "judge = DeepSearchQAJudge()\n",
    "\n",
    "console.print(\"[dim]Evaluating with LLM judge...[/dim]\\n\")\n",
    "\n",
    "# Get detailed evaluation\n",
    "score, result = judge.evaluate_with_details(\n",
    "    question=example.problem,\n",
    "    answer=eval_response.text,\n",
    "    ground_truth=example.answer,\n",
    "    answer_type=example.answer_type,\n",
    ")\n",
    "\n",
    "# Display results\n",
    "outcome_colors = {\n",
    "    \"fully_correct\": \"green\",\n",
    "    \"correct_with_extraneous\": \"yellow\",\n",
    "    \"partially_correct\": \"orange1\",\n",
    "    \"fully_incorrect\": \"red\",\n",
    "}\n",
    "outcome_color = outcome_colors.get(result.outcome, \"white\")\n",
    "\n",
    "metrics_table = Table(title=\"Evaluation Results\")\n",
    "metrics_table.add_column(\"Metric\", style=\"cyan\")\n",
    "metrics_table.add_column(\"Value\", style=\"white\")\n",
    "\n",
    "metrics_table.add_row(\"Outcome\", f\"[{outcome_color}]{result.outcome}[/{outcome_color}]\")\n",
    "metrics_table.add_row(\"Precision\", f\"{result.precision:.2f}\")\n",
    "metrics_table.add_row(\"Recall\", f\"{result.recall:.2f}\")\n",
    "metrics_table.add_row(\"F1 Score\", f\"[bold]{result.f1_score:.2f}[/bold]\")\n",
    "\n",
    "console.print(metrics_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-show-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the judge's explanation\n",
    "if result.explanation:\n",
    "    console.print(Panel(result.explanation, title=\"Judge's Explanation\", border_style=\"magenta\"))\n",
    "\n",
    "# Show correctness details if available\n",
    "if result.correctness_details:\n",
    "    details_table = Table(title=\"Correctness Details\")\n",
    "    details_table.add_column(\"Ground Truth Item\", style=\"white\")\n",
    "    details_table.add_column(\"Found\", style=\"cyan\", justify=\"center\")\n",
    "\n",
    "    for item, found in result.correctness_details.items():\n",
    "        icon = \"[green]✓[/green]\" if found else \"[red]✗[/red]\"\n",
    "        details_table.add_row(item[:50] + \"...\" if len(item) > 50 else item, icon)\n",
    "\n",
    "    console.print(details_table)\n",
    "\n",
    "# Show extraneous items if any\n",
    "if result.extraneous_items:\n",
    "    console.print(\n",
    "        Panel(\n",
    "            \"\\n\".join(f\"• {item}\" for item in result.extraneous_items),\n",
    "            title=\"Extraneous Items (not in ground truth)\",\n",
    "            border_style=\"yellow\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-cli-intro",
   "metadata": {},
   "source": [
    "## 5. Using the CLI\n",
    "\n",
    "You can also run the agent and evaluations from the command line:\n",
    "\n",
    "```bash\n",
    "# Ask a question\n",
    "knowledge-agent ask \"What is quantum computing?\" --show-plan\n",
    "\n",
    "# Run evaluation on specific example IDs\n",
    "knowledge-agent eval --ids 123 456 --show-plan\n",
    "\n",
    "# Run evaluation on random samples\n",
    "knowledge-agent eval --samples 5 --category \"Finance & Economics\"\n",
    "\n",
    "# View dataset samples\n",
    "knowledge-agent sample --category \"Science & Technology\" --count 3\n",
    "```\n",
    "\n",
    "The CLI shows a live display with:\n",
    "- Research plan checklist with step statuses\n",
    "- Tool calls as they happen\n",
    "- Context usage indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Agent Architecture** - Planning, execution, and reflection with multiple tools\n",
    "2. **Tools** - Google Search, web fetching, and file operations\n",
    "3. **Running the Agent** - How to ask questions and interpret responses\n",
    "4. **Evaluation** - Using DeepSearchQA and LLM-as-judge for quality assessment\n",
    "\n",
    "**Next**: In notebook 02, you'll learn about **Langfuse tracing** to observe agent behavior in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-done",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\n",
    "    Panel(\n",
    "        \"[green]✓[/green] Notebook complete!\\n\\n\"\n",
    "        \"[cyan]Next:[/cyan] Open [bold]02_langfuse_tracing.ipynb[/bold] to learn about observability.\",\n",
    "        title=\"Done\",\n",
    "        border_style=\"green\",\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
